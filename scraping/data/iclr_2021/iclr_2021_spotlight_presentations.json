[
      {
            "data_id": "pBqLS-7KYAF",
            "paper_title": "Sparse Quantized Spectral Clustering",
            "forum_link": "https://openreview.net/forum?id=pBqLS-7KYAF",
            "pdf_link": "https://openreview.net/pdf?id=pBqLS-7KYAF",
            "authors": [
                  "Zhenyu Liao",
                  "Romain Couillet",
                  "Michael W. Mahoney"
            ],
            "keywords": "Eigenspectrum, high-dimensional statistic, random matrix theory, spectral clustering",
            "abstract": "Given a large data matrix, sparsifying, quantizing, and/or performing other entry-wise nonlinear operations can have numerous benefits, ranging from speeding up iterative algorithms for core numerical linear algebra problems to providing nonlinear filters to design state-of-the-art neural network models. Here, we exploit tools from random matrix theory to make precise statements about how the eigenspectrum of a matrix changes under such nonlinear transformations. In particular, we show that very little change occurs in the informative eigenstructure, even under drastic sparsification/quantization, and consequently that very little downstream performance loss occurs when working with very aggressively sparsified or quantized spectral clustering problems.\n        We illustrate how these results depend on the nonlinearity, we characterize a phase transition beyond which spectral clustering becomes possible, and we show when such nonlinear transformations can introduce spurious non-informative eigenvectors.",
            "code-of-ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics"
      },
      {
            "data_id": "HHSEKOnPvaO",
            "paper_title": "Graph-Based Continual Learning",
            "forum_link": "https://openreview.net/forum?id=HHSEKOnPvaO",
            "pdf_link": "https://openreview.net/pdf?id=HHSEKOnPvaO",
            "authors": [
                  "Binh Tang",
                  "David S. Matteson"
            ],
            "abstract": "Despite significant advances, continual learning models still suffer from catastrophic forgetting when exposed to incrementally available data from non-stationary distributions. Rehearsal approaches alleviate the problem by maintaining and replaying a small episodic memory of previous samples, often implemented as an array of independent memory slots. In this work, we propose to augment such an array with a learnable random graph that captures pairwise similarities between its samples, and use it not only to learn new tasks but also to guard against forgetting. Empirical results on several benchmark datasets show that our model consistently outperforms recently proposed baselines for task-free continual learning.",
            "code-of-ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
            "supplementary-material": "<a href=\"/attachment?id=HHSEKOnPvaO&amp;name=supplementary_material\" class=\"attachment-download-link\" title=\"Download Supplementary Material\" target=\"_blank\"><span class=\"glyphicon glyphicon-download-alt\" aria-hidden=\"true\"></span> &nbsp;zip</a>"
      },
      {
            "data_id": "Vfs_2RnOD0H",
            "paper_title": "Dynamic Tensor Rematerialization",
            "forum_link": "https://openreview.net/forum?id=Vfs_2RnOD0H",
            "pdf_link": "https://openreview.net/pdf?id=Vfs_2RnOD0H",
            "authors": [
                  "Marisa Kirisame",
                  "Steven Lyubomirsky",
                  "Altan Haan",
                  "Jennifer Brennan",
                  "Mike He",
                  "Jared Roesch",
                  "Tianqi Chen",
                  "Zachary Tatlock"
            ],
            "keywords": "Rematerialization, Memory-saving, Runtime Systems, Checkpointing",
            "abstract": "Checkpointing enables the training of deep learning models under restricted memory budgets by freeing intermediate activations from memory and recomputing them on demand. Current checkpointing techniques statically plan these recomputations offline and assume static computation graphs. We demonstrate that a simple online algorithm can achieve comparable performance by introducing Dynamic Tensor Rematerialization (DTR), a greedy online algorithm for checkpointing that is extensible and general, is parameterized by eviction policy, and supports dynamic models. We prove that DTR can train an <mjx-container class=\"MathJax CtxtMenu_Attached_0\" jax=\"CHTML\" role=\"presentation\" tabindex=\"0\" ctxtmenu_counter=\"0\" style=\"font-size: 113.1%; position: relative;\"><mjx-math class=\"MJX-TEX\" aria-hidden=\"true\"><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D441 TEX-I\"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role=\"presentation\" unselectable=\"on\" display=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>N</mi></math></mjx-assistive-mml></mjx-container>-layer linear feedforward network on an  <mjx-container class=\"MathJax CtxtMenu_Attached_0\" jax=\"CHTML\" role=\"presentation\" tabindex=\"0\" ctxtmenu_counter=\"1\" style=\"font-size: 113.1%; position: relative;\"><mjx-math class=\"MJX-TEX\" aria-hidden=\"true\"><mjx-mi class=\"mjx-n\"><mjx-c class=\"mjx-c3A9\"></mjx-c></mjx-mi><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c28\"></mjx-c></mjx-mo><mjx-msqrt><mjx-sqrt><mjx-surd><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c221A\"></mjx-c></mjx-mo></mjx-surd><mjx-box style=\"padding-top: 0.166em;\"><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D441 TEX-I\"></mjx-c></mjx-mi></mjx-box></mjx-sqrt></mjx-msqrt><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c29\"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role=\"presentation\" unselectable=\"on\" display=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi mathvariant=\"normal\">\u03a9</mi><mo stretchy=\"false\">(</mo><msqrt><mi>N</mi></msqrt><mo stretchy=\"false\">)</mo></math></mjx-assistive-mml></mjx-container> memory budget with only <mjx-container class=\"MathJax CtxtMenu_Attached_0\" jax=\"CHTML\" role=\"presentation\" tabindex=\"0\" ctxtmenu_counter=\"2\" style=\"font-size: 113.1%; position: relative;\"><mjx-math class=\"MJX-TEX\" aria-hidden=\"true\"><mjx-texatom texclass=\"ORD\"><mjx-mi class=\"mjx-cal mjx-i\"><mjx-c class=\"mjx-c4F TEX-C\"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c28\"></mjx-c></mjx-mo><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D441 TEX-I\"></mjx-c></mjx-mi><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c29\"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role=\"presentation\" unselectable=\"on\" display=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi data-mjx-variant=\"-tex-calligraphic\" mathvariant=\"script\">O</mi></mrow><mo stretchy=\"false\">(</mo><mi>N</mi><mo stretchy=\"false\">)</mo></math></mjx-assistive-mml></mjx-container> tensor operations. DTR closely matches the performance of optimal static checkpointing in simulated experiments. We incorporate a DTR prototype into PyTorch merely by interposing on tensor allocations and operator calls and collecting lightweight metadata on tensors.",
            "one-sentence-summary": "We present an online algorithm for rematerialization (recomputing intermediate activations during backpropagation instead of storing them), which enables training under low memory, finding that it is competitive with offline techniques.",
            "code-of-ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
            "supplementary-material": "<a href=\"/attachment?id=Vfs_2RnOD0H&amp;name=supplementary_material\" class=\"attachment-download-link\" title=\"Download Supplementary Material\" target=\"_blank\"><span class=\"glyphicon glyphicon-download-alt\" aria-hidden=\"true\"></span> &nbsp;zip</a>"
      },
      {
            "data_id": "F1vEjWK-lH_",
            "paper_title": "Gradient Vaccine: Investigating and Improving Multi-task Optimization in Massively Multilingual Models",
            "forum_link": "https://openreview.net/forum?id=F1vEjWK-lH_",
            "pdf_link": "https://openreview.net/pdf?id=F1vEjWK-lH_",
            "authors": [
                  "Zirui Wang",
                  "Yulia Tsvetkov",
                  "Orhan Firat",
                  "Yuan Cao"
            ],
            "keywords": "Multi-task Learning, Multilingual Modeling",
            "abstract": "Massively multilingual models subsuming tens or even hundreds of languages pose great challenges to multi-task optimization. While it is a common practice to apply a language-agnostic procedure optimizing a joint multilingual task objective, how to properly characterize and take advantage of its underlying problem structure for improving optimization efficiency remains under-explored. In this paper, we attempt to peek into the black-box of multilingual optimization through the lens of loss function geometry. We find that gradient similarity measured along the optimization trajectory is an important signal, which correlates well with not only language proximity but also the overall model performance. Such observation helps us to identify a critical limitation of existing gradient-based multi-task learning methods, and thus we derive a simple and scalable optimization procedure, named Gradient Vaccine, which encourages more geometrically aligned parameter updates for close tasks. Empirically, our method obtains significant model performance gains on multilingual machine translation and XTREME benchmark tasks for multilingual language models. Our work reveals the importance of properly measuring and utilizing language proximity in multilingual optimization, and has broader implications for multi-task learning beyond multilingual modeling.",
            "code-of-ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics"
      },
      {
            "data_id": "87ZwsaQNHPZ",
            "paper_title": "CPT: Efficient Deep Neural Network Training via Cyclic Precision",
            "forum_link": "https://openreview.net/forum?id=87ZwsaQNHPZ",
            "pdf_link": "https://openreview.net/pdf?id=87ZwsaQNHPZ",
            "authors": [
                  "Yonggan Fu",
                  "Han Guo",
                  "Meng Li",
                  "Xin Yang",
                  "Yining Ding",
                  "Vikas Chandra",
                  "Yingyan Lin"
            ],
            "keywords": "Efficient training, low precision training",
            "abstract": "Low-precision deep neural network (DNN) training has gained tremendous attention as reducing precision is one of the most effective knobs for boosting DNNs' training time/energy efficiency. In this paper, we attempt to explore low-precision training from a new perspective as inspired by recent findings in understanding DNN training: we conjecture that DNNs' precision might have a similar effect as the learning rate during DNN training, and advocate dynamic precision along the training trajectory for further boosting the time/energy efficiency of DNN training. Specifically, we propose Cyclic Precision Training (CPT) to cyclically vary the precision between two boundary values which can be identified using a simple precision range test within the first few training epochs. Extensive simulations and ablation studies on five datasets and eleven models demonstrate that CPT's effectiveness is consistent across various models/tasks (including classification and language modeling). Furthermore, through experiments and visualization we show that CPT helps to (1) converge to a wider minima with a lower generalization error and (2) reduce training variance which we believe opens up a new design knob for simultaneously improving the optimization and efficiency of DNN training.",
            "one-sentence-summary": "We propose Cyclic Precision Training towards better accuracy-efficiency trade-offs in DNN training.",
            "code-of-ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics"
      },
      {
            "data_id": "04LZCAxMSco",
            "paper_title": "Learning a Latent Simplex in Input Sparsity Time",
            "forum_link": "https://openreview.net/forum?id=04LZCAxMSco",
            "pdf_link": "https://openreview.net/pdf?id=04LZCAxMSco",
            "authors": [
                  "Ainesh Bakshi",
                  "Chiranjib Bhattacharyya",
                  "Ravi Kannan",
                  "David Woodruff",
                  "Samson Zhou"
            ],
            "keywords": "Latent Simplex, numerical linear algebra, low-rank approximation",
            "abstract": "We consider the problem of learning a latent <mjx-container class=\"MathJax CtxtMenu_Attached_0\" jax=\"CHTML\" role=\"presentation\" tabindex=\"0\" ctxtmenu_counter=\"3\" style=\"font-size: 113.1%; position: relative;\"><mjx-math class=\"MJX-TEX\" aria-hidden=\"true\"><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D458 TEX-I\"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role=\"presentation\" unselectable=\"on\" display=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></mjx-assistive-mml></mjx-container>-vertex simplex <mjx-container class=\"MathJax CtxtMenu_Attached_0\" jax=\"CHTML\" role=\"presentation\" tabindex=\"0\" ctxtmenu_counter=\"4\" style=\"font-size: 113.1%; position: relative;\"><mjx-math class=\"MJX-TEX\" aria-hidden=\"true\"><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D43E TEX-I\"></mjx-c></mjx-mi><mjx-mo class=\"mjx-n\" space=\"4\"><mjx-c class=\"mjx-c2208\"></mjx-c></mjx-mo><mjx-msup space=\"4\"><mjx-texatom texclass=\"ORD\"><mjx-mi class=\"mjx-ds mjx-b\"><mjx-c class=\"mjx-c211D TEX-A\"></mjx-c></mjx-mi></mjx-texatom><mjx-script style=\"vertical-align: 0.41em;\"><mjx-mi class=\"mjx-i\" size=\"s\"><mjx-c class=\"mjx-c1D451 TEX-I\"></mjx-c></mjx-mi></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml role=\"presentation\" unselectable=\"on\" display=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>K</mi><mo>\u2208</mo><msup><mrow><mi mathvariant=\"double-struck\">R</mi></mrow><mi>d</mi></msup></math></mjx-assistive-mml></mjx-container>, given <mjx-container class=\"MathJax CtxtMenu_Attached_0\" jax=\"CHTML\" role=\"presentation\" tabindex=\"0\" ctxtmenu_counter=\"5\" style=\"font-size: 113.1%; position: relative;\"><mjx-math class=\"MJX-TEX\" aria-hidden=\"true\"><mjx-texatom texclass=\"ORD\"><mjx-mi class=\"mjx-b\"><mjx-c class=\"mjx-c1D400 TEX-B\"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class=\"mjx-n\" space=\"4\"><mjx-c class=\"mjx-c2208\"></mjx-c></mjx-mo><mjx-msup space=\"4\"><mjx-texatom texclass=\"ORD\"><mjx-mi class=\"mjx-ds mjx-b\"><mjx-c class=\"mjx-c211D TEX-A\"></mjx-c></mjx-mi></mjx-texatom><mjx-script style=\"vertical-align: 0.41em;\"><mjx-texatom size=\"s\" texclass=\"ORD\"><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D451 TEX-I\"></mjx-c></mjx-mi><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-cD7\"></mjx-c></mjx-mo><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D45B TEX-I\"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml role=\"presentation\" unselectable=\"on\" display=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi mathvariant=\"bold\">A</mi></mrow><mo>\u2208</mo><msup><mrow><mi mathvariant=\"double-struck\">R</mi></mrow><mrow><mi>d</mi><mo>\u00d7</mo><mi>n</mi></mrow></msup></math></mjx-assistive-mml></mjx-container>, which can be viewed as <mjx-container class=\"MathJax CtxtMenu_Attached_0\" jax=\"CHTML\" role=\"presentation\" tabindex=\"0\" ctxtmenu_counter=\"6\" style=\"font-size: 113.1%; position: relative;\"><mjx-math class=\"MJX-TEX\" aria-hidden=\"true\"><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D45B TEX-I\"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role=\"presentation\" unselectable=\"on\" display=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>n</mi></math></mjx-assistive-mml></mjx-container> data points that are formed by randomly perturbing some latent points in <mjx-container class=\"MathJax CtxtMenu_Attached_0\" jax=\"CHTML\" role=\"presentation\" tabindex=\"0\" ctxtmenu_counter=\"7\" style=\"font-size: 113.1%; position: relative;\"><mjx-math class=\"MJX-TEX\" aria-hidden=\"true\"><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D43E TEX-I\"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role=\"presentation\" unselectable=\"on\" display=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>K</mi></math></mjx-assistive-mml></mjx-container>, possibly beyond <mjx-container class=\"MathJax CtxtMenu_Attached_0\" jax=\"CHTML\" role=\"presentation\" tabindex=\"0\" ctxtmenu_counter=\"8\" style=\"font-size: 113.1%; position: relative;\"><mjx-math class=\"MJX-TEX\" aria-hidden=\"true\"><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D43E TEX-I\"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role=\"presentation\" unselectable=\"on\" display=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>K</mi></math></mjx-assistive-mml></mjx-container>. A large class of latent variable models, such as adversarial clustering, mixed membership stochastic block models, and topic models can be cast in this view of learning a latent simplex. Bhattacharyya and Kannan (SODA 2020) give an algorithm for learning such a <mjx-container class=\"MathJax CtxtMenu_Attached_0\" jax=\"CHTML\" role=\"presentation\" tabindex=\"0\" ctxtmenu_counter=\"9\" style=\"font-size: 113.1%; position: relative;\"><mjx-math class=\"MJX-TEX\" aria-hidden=\"true\"><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D458 TEX-I\"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role=\"presentation\" unselectable=\"on\" display=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></mjx-assistive-mml></mjx-container>-vertex latent simplex in time roughly <mjx-container class=\"MathJax CtxtMenu_Attached_0\" jax=\"CHTML\" role=\"presentation\" tabindex=\"0\" ctxtmenu_counter=\"10\" style=\"font-size: 113.1%; position: relative;\"><mjx-math class=\"MJX-TEX\" aria-hidden=\"true\"><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D442 TEX-I\"></mjx-c></mjx-mi><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c28\"></mjx-c></mjx-mo><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D458 TEX-I\"></mjx-c></mjx-mi><mjx-mo class=\"mjx-n\" space=\"3\"><mjx-c class=\"mjx-c22C5\"></mjx-c></mjx-mo><mjx-mtext class=\"mjx-n\" space=\"3\"><mjx-c class=\"mjx-c6E\"></mjx-c><mjx-c class=\"mjx-c6E\"></mjx-c><mjx-c class=\"mjx-c7A\"></mjx-c></mjx-mtext><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c28\"></mjx-c></mjx-mo><mjx-texatom texclass=\"ORD\"><mjx-mi class=\"mjx-b\"><mjx-c class=\"mjx-c1D400 TEX-B\"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c29\"></mjx-c></mjx-mo><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c29\"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role=\"presentation\" unselectable=\"on\" display=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>O</mi><mo stretchy=\"false\">(</mo><mi>k</mi><mo>\u22c5</mo><mtext>nnz</mtext><mo stretchy=\"false\">(</mo><mrow><mi mathvariant=\"bold\">A</mi></mrow><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></math></mjx-assistive-mml></mjx-container>, where <mjx-container class=\"MathJax CtxtMenu_Attached_0\" jax=\"CHTML\" role=\"presentation\" tabindex=\"0\" ctxtmenu_counter=\"11\" style=\"font-size: 113.1%; position: relative;\"><mjx-math class=\"MJX-TEX\" aria-hidden=\"true\"><mjx-mtext class=\"mjx-n\"><mjx-c class=\"mjx-c6E\"></mjx-c><mjx-c class=\"mjx-c6E\"></mjx-c><mjx-c class=\"mjx-c7A\"></mjx-c></mjx-mtext><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c28\"></mjx-c></mjx-mo><mjx-texatom texclass=\"ORD\"><mjx-mi class=\"mjx-b\"><mjx-c class=\"mjx-c1D400 TEX-B\"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c29\"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role=\"presentation\" unselectable=\"on\" display=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mtext>nnz</mtext><mo stretchy=\"false\">(</mo><mrow><mi mathvariant=\"bold\">A</mi></mrow><mo stretchy=\"false\">)</mo></math></mjx-assistive-mml></mjx-container> is the number of non-zeros in <mjx-container class=\"MathJax CtxtMenu_Attached_0\" jax=\"CHTML\" role=\"presentation\" tabindex=\"0\" ctxtmenu_counter=\"12\" style=\"font-size: 113.1%; position: relative;\"><mjx-math class=\"MJX-TEX\" aria-hidden=\"true\"><mjx-texatom texclass=\"ORD\"><mjx-mi class=\"mjx-b\"><mjx-c class=\"mjx-c1D400 TEX-B\"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml role=\"presentation\" unselectable=\"on\" display=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi mathvariant=\"bold\">A</mi></mrow></math></mjx-assistive-mml></mjx-container>. We show that the dependence on <mjx-container class=\"MathJax CtxtMenu_Attached_0\" jax=\"CHTML\" role=\"presentation\" tabindex=\"0\" ctxtmenu_counter=\"13\" style=\"font-size: 113.1%; position: relative;\"><mjx-math class=\"MJX-TEX\" aria-hidden=\"true\"><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D458 TEX-I\"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role=\"presentation\" unselectable=\"on\" display=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></mjx-assistive-mml></mjx-container> in the running time is unnecessary given a natural assumption about the mass of the top <mjx-container class=\"MathJax CtxtMenu_Attached_0\" jax=\"CHTML\" role=\"presentation\" tabindex=\"0\" ctxtmenu_counter=\"14\" style=\"font-size: 113.1%; position: relative;\"><mjx-math class=\"MJX-TEX\" aria-hidden=\"true\"><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D458 TEX-I\"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role=\"presentation\" unselectable=\"on\" display=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></mjx-assistive-mml></mjx-container> singular values of <mjx-container class=\"MathJax CtxtMenu_Attached_0\" jax=\"CHTML\" role=\"presentation\" tabindex=\"0\" ctxtmenu_counter=\"15\" style=\"font-size: 113.1%; position: relative;\"><mjx-math class=\"MJX-TEX\" aria-hidden=\"true\"><mjx-texatom texclass=\"ORD\"><mjx-mi class=\"mjx-b\"><mjx-c class=\"mjx-c1D400 TEX-B\"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml role=\"presentation\" unselectable=\"on\" display=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi mathvariant=\"bold\">A</mi></mrow></math></mjx-assistive-mml></mjx-container>, which holds in many of these applications. Further, we show this assumption is necessary, as otherwise an algorithm for learning a latent simplex would imply a better low rank approximation algorithm than what is known. \n        \n        We obtain a spectral low-rank approximation to <mjx-container class=\"MathJax CtxtMenu_Attached_0\" jax=\"CHTML\" role=\"presentation\" tabindex=\"0\" ctxtmenu_counter=\"16\" style=\"font-size: 113.1%; position: relative;\"><mjx-math class=\"MJX-TEX\" aria-hidden=\"true\"><mjx-texatom texclass=\"ORD\"><mjx-mi class=\"mjx-b\"><mjx-c class=\"mjx-c1D400 TEX-B\"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml role=\"presentation\" unselectable=\"on\" display=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi mathvariant=\"bold\">A</mi></mrow></math></mjx-assistive-mml></mjx-container> in input-sparsity time and show that the column space thus obtained has small <mjx-container class=\"MathJax CtxtMenu_Attached_0\" jax=\"CHTML\" role=\"presentation\" tabindex=\"0\" ctxtmenu_counter=\"17\" style=\"font-size: 113.1%; position: relative;\"><mjx-math class=\"MJX-TEX\" aria-hidden=\"true\"><mjx-mi class=\"mjx-n\"><mjx-c class=\"mjx-c73\"></mjx-c><mjx-c class=\"mjx-c69\"></mjx-c><mjx-c class=\"mjx-c6E\"></mjx-c></mjx-mi><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c2061\"></mjx-c></mjx-mo><mjx-mi class=\"mjx-n\" space=\"2\"><mjx-c class=\"mjx-c398\"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role=\"presentation\" unselectable=\"on\" display=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>sin</mi><mo data-mjx-texclass=\"NONE\">\u2061</mo><mi mathvariant=\"normal\">\u0398</mi></math></mjx-assistive-mml></mjx-container> (angular) distance to the right top-<mjx-container class=\"MathJax CtxtMenu_Attached_0\" jax=\"CHTML\" role=\"presentation\" tabindex=\"0\" ctxtmenu_counter=\"18\" style=\"font-size: 113.1%; position: relative;\"><mjx-math class=\"MJX-TEX\" aria-hidden=\"true\"><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D458 TEX-I\"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role=\"presentation\" unselectable=\"on\" display=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></mjx-assistive-mml></mjx-container> singular space of <mjx-container class=\"MathJax CtxtMenu_Attached_0\" jax=\"CHTML\" role=\"presentation\" tabindex=\"0\" ctxtmenu_counter=\"19\" style=\"font-size: 113.1%; position: relative;\"><mjx-math class=\"MJX-TEX\" aria-hidden=\"true\"><mjx-texatom texclass=\"ORD\"><mjx-mi class=\"mjx-b\"><mjx-c class=\"mjx-c1D400 TEX-B\"></mjx-c></mjx-mi></mjx-texatom></mjx-math><mjx-assistive-mml role=\"presentation\" unselectable=\"on\" display=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi mathvariant=\"bold\">A</mi></mrow></math></mjx-assistive-mml></mjx-container>. Our algorithm then selects <mjx-container class=\"MathJax CtxtMenu_Attached_0\" jax=\"CHTML\" role=\"presentation\" tabindex=\"0\" ctxtmenu_counter=\"20\" style=\"font-size: 113.1%; position: relative;\"><mjx-math class=\"MJX-TEX\" aria-hidden=\"true\"><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D458 TEX-I\"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role=\"presentation\" unselectable=\"on\" display=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></mjx-assistive-mml></mjx-container> points in the low-rank  subspace with the largest inner product (in absolute value) with <mjx-container class=\"MathJax CtxtMenu_Attached_0\" jax=\"CHTML\" role=\"presentation\" tabindex=\"0\" ctxtmenu_counter=\"21\" style=\"font-size: 113.1%; position: relative;\"><mjx-math class=\"MJX-TEX\" aria-hidden=\"true\"><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D458 TEX-I\"></mjx-c></mjx-mi></mjx-math><mjx-assistive-mml role=\"presentation\" unselectable=\"on\" display=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi>k</mi></math></mjx-assistive-mml></mjx-container> carefully chosen random vectors. By working in the low-rank subspace, we avoid reading the entire matrix in each iteration and thus circumvent the <mjx-container class=\"MathJax CtxtMenu_Attached_0\" jax=\"CHTML\" role=\"presentation\" tabindex=\"0\" ctxtmenu_counter=\"22\" style=\"font-size: 113.1%; position: relative;\"><mjx-math class=\"MJX-TEX\" aria-hidden=\"true\"><mjx-mi class=\"mjx-n\"><mjx-c class=\"mjx-c398\"></mjx-c></mjx-mi><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c28\"></mjx-c></mjx-mo><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D458 TEX-I\"></mjx-c></mjx-mi><mjx-mo class=\"mjx-n\" space=\"3\"><mjx-c class=\"mjx-c22C5\"></mjx-c></mjx-mo><mjx-mtext class=\"mjx-n\" space=\"3\"><mjx-c class=\"mjx-c6E\"></mjx-c><mjx-c class=\"mjx-c6E\"></mjx-c><mjx-c class=\"mjx-c7A\"></mjx-c></mjx-mtext><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c28\"></mjx-c></mjx-mo><mjx-texatom texclass=\"ORD\"><mjx-mi class=\"mjx-b\"><mjx-c class=\"mjx-c1D400 TEX-B\"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c29\"></mjx-c></mjx-mo><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c29\"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml role=\"presentation\" unselectable=\"on\" display=\"inline\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mi mathvariant=\"normal\">\u0398</mi><mo stretchy=\"false\">(</mo><mi>k</mi><mo>\u22c5</mo><mtext>nnz</mtext><mo stretchy=\"false\">(</mo><mrow><mi mathvariant=\"bold\">A</mi></mrow><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></math></mjx-assistive-mml></mjx-container> running time.",
            "one-sentence-summary": "We obtain the first input sparsity runtime algorithm for the problem of learning a latent simplex.",
            "code-of-ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
            "supplementary-material": "<a href=\"/attachment?id=04LZCAxMSco&amp;name=supplementary_material\" class=\"attachment-download-link\" title=\"Download Supplementary Material\" target=\"_blank\"><span class=\"glyphicon glyphicon-download-alt\" aria-hidden=\"true\"></span> &nbsp;zip</a>"
      },
      {
            "data_id": "lxHgXYN4bwl",
            "paper_title": "Expressive Power of Invariant and Equivariant Graph Neural Networks",
            "forum_link": "https://openreview.net/forum?id=lxHgXYN4bwl",
            "pdf_link": "https://openreview.net/pdf?id=lxHgXYN4bwl",
            "authors": [
                  "Waiss Azizian",
                  "marc lelarge"
            ],
            "keywords": "Graph Neural Network, Universality, Approximation",
            "abstract": "Various classes of Graph Neural Networks (GNN) have been proposed and shown to be successful in a wide range of applications with graph structured data. In this paper, we propose a theoretical framework able to compare the expressive power of these GNN architectures. The current universality theorems only apply to intractable classes of GNNs. Here, we prove the first approximation guarantees for practical GNNs, paving the way for a better understanding of their generalization. Our theoretical results are proved for invariant GNNs computing a graph embedding (permutation of the nodes of the input graph does not affect the output) and equivariant GNNs computing an embedding of the nodes (permutation of the input permutes the output). We show that Folklore Graph Neural Networks (FGNN), which are tensor based GNNs augmented with matrix multiplication are the most expressive architectures proposed so far for a given tensor order. We illustrate our results on the Quadratic Assignment Problem (a NP-Hard combinatorial problem) by showing that FGNNs are able to learn how to solve the problem, leading to much better average performances than existing algorithms (based on spectral, SDP or other GNNs architectures). On a practical side, we also implement masked tensors to handle batches of graphs of varying sizes.",
            "code-of-ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
            "supplementary-material": "<a href=\"/attachment?id=lxHgXYN4bwl&amp;name=supplementary_material\" class=\"attachment-download-link\" title=\"Download Supplementary Material\" target=\"_blank\"><span class=\"glyphicon glyphicon-download-alt\" aria-hidden=\"true\"></span> &nbsp;zip</a>"
      },
      {
            "data_id": "PUkhWz65dy5",
            "paper_title": "Discovering a set of policies for the worst case reward",
            "forum_link": "https://openreview.net/forum?id=PUkhWz65dy5",
            "pdf_link": "https://openreview.net/pdf?id=PUkhWz65dy5",
            "authors": [
                  "Tom Zahavy",
                  "Andre Barreto",
                  "Daniel J Mankowitz",
                  "Shaobo Hou",
                  "Brendan O'Donoghue",
                  "Iurii Kemaev",
                  "Satinder Singh"
            ],
            "abstract": "We study the problem of how to construct a set of policies that can be composed together to solve a collection of reinforcement learning tasks. Each task is a different reward function defined as a linear combination of  known features. We consider a specific class of policy compositions which we call set improving policies (SIPs): given a set of policies and a set of tasks, a SIP is any composition of the former whose performance is at least as good as that of its constituents across all the tasks. We focus on the most conservative instantiation of SIPs, set-max policies (SMPs), so our analysis extends to any SIP. This includes known policy-composition operators like generalized policy improvement. Our main contribution is an algorithm that builds a set of policies in order to maximize the worst-case performance of the resulting SMP on the set of tasks. The algorithm works by successively adding new policies to the set. We show that the worst-case performance of the resulting SMP strictly improves at each iteration, and the algorithm only stops when there does not exist a policy that leads to improved performance. We empirically evaluate our algorithm on a grid world and also on a set of domains from the DeepMind control suite. We confirm our theoretical results regarding the monotonically improving performance of our algorithm. Interestingly, we also show empirically that the sets of policies computed by the algorithm are diverse, leading to different trajectories in the grid world and very distinct locomotion skills in the control suite.",
            "code-of-ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics",
            "supplementary-material": "<a href=\"/attachment?id=PUkhWz65dy5&amp;name=supplementary_material\" class=\"attachment-download-link\" title=\"Download Supplementary Material\" target=\"_blank\"><span class=\"glyphicon glyphicon-download-alt\" aria-hidden=\"true\"></span> &nbsp;zip</a>",
            "one-sentence-summary": "Discovering a set of diverse RL policies by optimising the robustness of the set"
      },
      {
            "data_id": "UcoXdfrORC",
            "paper_title": "Model-Based Visual Planning with Self-Supervised Functional Distances",
            "forum_link": "https://openreview.net/forum?id=UcoXdfrORC",
            "pdf_link": "https://openreview.net/pdf?id=UcoXdfrORC",
            "authors": [
                  "Stephen Tian",
                  "Suraj Nair",
                  "Frederik Ebert",
                  "Sudeep Dasari",
                  "Benjamin Eysenbach",
                  "Chelsea Finn",
                  "Sergey Levine"
            ],
            "keywords": "planning, model learning, distance learning, reinforcement learning, robotics",
            "abstract": "A generalist robot must be able to complete a variety of tasks in its environment. One appealing way to specify each task is in terms of a goal observation. However, learning goal-reaching policies with reinforcement learning remains a challenging problem, particularly when hand-engineered reward functions are not available. Learned dynamics models are a promising approach for learning about the environment without rewards or task-directed data, but planning to reach goals with such a model requires a notion of functional similarity between observations and goal states. We present a self-supervised method for model-based visual goal reaching, which uses both a visual dynamics model as well as a dynamical distance function learned using model-free reinforcement learning. Our approach learns entirely using offline, unlabeled data, making it practical to scale to large and diverse datasets. In our experiments, we find that our method can successfully learn models that perform a variety of tasks at test-time, moving objects amid distractors with a simulated robotic arm and even learning to open and close a drawer using a real-world robot. In comparisons, we find that this approach substantially outperforms both model-free and model-based prior methods.",
            "one-sentence-summary": "We combine model-based planning with dynamical distance learning to solve visual goal-reaching tasks, using random, unlabeled, experience.",
            "code-of-ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics"
      },
      {
            "data_id": "80FMcTSZ6J0",
            "paper_title": "Noise against noise: stochastic label noise helps combat inherent label noise",
            "forum_link": "https://openreview.net/forum?id=80FMcTSZ6J0",
            "pdf_link": "https://openreview.net/pdf?id=80FMcTSZ6J0",
            "authors": [
                  "Pengfei Chen",
                  "Guangyong Chen",
                  "Junjie Ye",
                  "jingwei zhao",
                  "Pheng-Ann Heng"
            ],
            "keywords": "Noisy Labels, Robust Learning, SGD noise, Regularization",
            "abstract": "The noise in stochastic gradient descent (SGD) provides a crucial implicit regularization effect, previously studied in optimization by analyzing the dynamics of parameter updates. In this paper, we are interested in learning with noisy labels, where we have a collection of samples with potential mislabeling. We show that a previously rarely discussed SGD noise, induced by stochastic label noise (SLN), mitigates the effects of inherent label noise. In contrast, the common SGD noise directly applied to model parameters does not. We formalize the differences and connections of SGD noise variants, showing that SLN induces SGD noise dependent on the sharpness of output landscape and the confidence of output probability, which may help escape from sharp minima and prevent overconfidence. SLN not only improves generalization in its simplest form but also boosts popular robust training methods, including sample selection and label correction. Specifically, we present an enhanced algorithm by applying SLN to label correction. Our code is released.",
            "one-sentence-summary": "SGD noise induced by stochastic label noise helps escape sharp minima and prevents overconfidence, hence can mitigate the effects of inherent label noise and improve generalization.",
            "supplementary-material": "<a href=\"/attachment?id=80FMcTSZ6J0&amp;name=supplementary_material\" class=\"attachment-download-link\" title=\"Download Supplementary Material\" target=\"_blank\"><span class=\"glyphicon glyphicon-download-alt\" aria-hidden=\"true\"></span> &nbsp;zip</a>",
            "code-of-ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics"
      }
]